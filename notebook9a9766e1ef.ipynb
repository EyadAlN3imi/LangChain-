{"cells":[{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T19:10:25.037011Z","iopub.status.busy":"2024-09-18T19:10:25.036602Z","iopub.status.idle":"2024-09-18T19:10:25.054290Z","shell.execute_reply":"2024-09-18T19:10:25.053369Z","shell.execute_reply.started":"2024-09-18T19:10:25.036971Z"},"trusted":true},"outputs":[],"source":["from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n","from langchain.llms import HuggingFacePipeline"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T18:59:30.151800Z","iopub.status.busy":"2024-09-18T18:59:30.151403Z","iopub.status.idle":"2024-09-18T18:59:30.156057Z","shell.execute_reply":"2024-09-18T18:59:30.155315Z","shell.execute_reply.started":"2024-09-18T18:59:30.151762Z"},"trusted":true},"outputs":[],"source":["model_id = 'chavinlo/alpaca-native'"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T18:59:46.423075Z","iopub.status.busy":"2024-09-18T18:59:46.422092Z","iopub.status.idle":"2024-09-18T19:04:37.946850Z","shell.execute_reply":"2024-09-18T19:04:37.946100Z","shell.execute_reply.started":"2024-09-18T18:59:46.423019Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8009c4a7dc54d5a9439df6c2aef539c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"daf08d497a2940598a7189f48004f076","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ae8ff9526e7484d912dae1f8011e942","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2192dd68f0304650934c14f3b829c310","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a90260450c0e42da9c936113d9cd4b52","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/556 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad21f2bfd0524befbb20d79cdda383d2","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a949cad30584e33aafd6f01abe47938","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52da55cc8b9942979938b944002515a8","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9c96833e50d4b82bc41e0040d5d2c05","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"043f6704baa04ee38a7a541b12f1aa56","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:546: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly by `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation, and ensure your `input_ids` input does not have negative values.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7262a45acd342f780c5601ff81cd57e","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15848020f94b4d6b963c032fd91c9142","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = LlamaTokenizer.from_pretrained(model_id)\n","\n","base_modle = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit= True, device_map='auto')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T19:07:58.237195Z","iopub.status.busy":"2024-09-18T19:07:58.236273Z","iopub.status.idle":"2024-09-18T19:07:58.241882Z","shell.execute_reply":"2024-09-18T19:07:58.240834Z","shell.execute_reply.started":"2024-09-18T19:07:58.237128Z"},"trusted":true},"outputs":[],"source":["hf_pipeline = pipeline('text-generation',\n","                     model=base_modle,\n","                     tokenizer=tokenizer,\n","                     max_length=256)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T19:10:48.673418Z","iopub.status.busy":"2024-09-18T19:10:48.672695Z","iopub.status.idle":"2024-09-18T19:10:48.677512Z","shell.execute_reply":"2024-09-18T19:10:48.676476Z","shell.execute_reply.started":"2024-09-18T19:10:48.673376Z"},"trusted":true},"outputs":[],"source":["llm = HuggingFacePipeline(pipeline=hf_pipeline)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T19:18:39.411729Z","iopub.status.busy":"2024-09-18T19:18:39.410926Z","iopub.status.idle":"2024-09-18T19:18:39.416025Z","shell.execute_reply":"2024-09-18T19:18:39.414984Z","shell.execute_reply.started":"2024-09-18T19:18:39.411686Z"},"trusted":true},"outputs":[],"source":["prompt = \"\"\"Below is an instruction that describes a task. Write a response that approprialtely completes the request.\n","###Instruction:\n","Suggest 2 ways to lose my weight.\n","\n","Answer:\n","\n","\"\"\".strip()"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-09-18T19:18:52.822958Z","iopub.status.busy":"2024-09-18T19:18:52.822048Z","iopub.status.idle":"2024-09-18T19:19:03.788241Z","shell.execute_reply":"2024-09-18T19:19:03.787174Z","shell.execute_reply.started":"2024-09-18T19:18:52.822917Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_36/700263155.py:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n","  print(llm(prompt))\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Below is an instruction that describes a task. Write a response that approprialtely completes the request.\n","###Instruction:\n","Suggest 2 ways to lose my weight.\n","\n","Answer: \n","1. Eat a balanced diet and exercise regularly.\n","2. Cut down on processed foods and sugary drinks, and increase your intake of fresh fruits and vegetables.\n"]}],"source":["print(llm(prompt))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
